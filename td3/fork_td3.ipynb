{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t2sTQCSE__pF"
      },
      "outputs": [],
      "source": [
        "# References: https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93 [1]\n",
        "# References: https://github.com/honghaow/FORK [2]\n",
        "\n",
        "%%capture\n",
        "!apt update\n",
        "!pip install 'gym[box2d]'\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "%matplotlib inline\n",
        "\n",
        "display = Display(visible=0,size=(600,600))\n",
        "display.start()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "plot_interval = 10 # update the plot every N episodes\n",
        "video_every = 20 # videos can take a very long time to render so only do it every N episodes\n",
        "max_episodes = 8000 # train longer for hardcore environment\n",
        "seed = 42 # for result replication\n",
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "# env = gym.make(\"BipedalWalkerHardcore-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWOoPFRj7cqa",
        "outputId": "c2eaed90-da7a-4df5-bb64-48129efe1d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ufal.pybox2d in /usr/local/lib/python3.10/dist-packages (2.3.10.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ufal.pybox2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wguo4xsxAIxM"
      },
      "outputs": [],
      "source": [
        "# Actor network architecture\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, action_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.max_action * self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8qgrxwXeAK3V"
      },
      "outputs": [],
      "source": [
        "# Critic network architectures\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Q1\n",
        "        self.q1 = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, 1)\n",
        "        )\n",
        "\n",
        "        # Q2\n",
        "        self.q2 = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "        x1 = self.q1(xu)\n",
        "        x2 = self.q2(xu)\n",
        "        return x1, x2\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "        return self.q1(xu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bmPJkXYPAOT4"
      },
      "outputs": [],
      "source": [
        "# Forward-Looking network achitecture\n",
        "class FORK(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(FORK, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "\n",
        "          nn.Linear(state_dim + action_dim, 400),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(400, 300),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(300, state_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x, u], 1)\n",
        "        return self.net(xu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UpTh3Ki2AQSm"
      },
      "outputs": [],
      "source": [
        "# Original code: https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
        "\n",
        "# Expects tuples of (state, next_state, action, reward, done)\n",
        "class ReplayBuffer(object):\n",
        "    \"\"\"Buffer to store tuples of experience replay\"\"\"\n",
        "\n",
        "    def __init__(self, max_size=1000000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_size (int): total amount of tuples to store\n",
        "        \"\"\"\n",
        "\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "\n",
        "    def add(self, data):\n",
        "        \"\"\"Add experience tuples to buffer\n",
        "\n",
        "        Args:\n",
        "            data (tuple): experience replay tuple\n",
        "        \"\"\"\n",
        "\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = data\n",
        "            self.ptr = (self.ptr + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(data)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samples a random amount of experiences from buffer of batch size\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): size of sample\n",
        "        \"\"\"\n",
        "\n",
        "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        states, actions, next_states, rewards, dones = [], [], [], [], []\n",
        "\n",
        "        for i in ind:\n",
        "            s, a, s_, r, d = self.storage[i]\n",
        "            states.append(np.array(s, copy=False))\n",
        "            actions.append(np.array(a, copy=False))\n",
        "            next_states.append(np.array(s_, copy=False))\n",
        "            rewards.append(np.array(r, copy=False))\n",
        "            dones.append(np.array(d, copy=False))\n",
        "\n",
        "        return np.array(states), np.array(actions), np.array(next_states), np.array(rewards).reshape(-1, 1), np.array(dones).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d0MHGk71ARBW"
      },
      "outputs": [],
      "source": [
        "class Agent(object):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action, max_state, env):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.fork = FORK(state_dim, action_dim).to(device)\n",
        "        self.fork_optimizer = torch.optim.Adam(self.fork.parameters(), lr=1e-3)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.max_state = max_state\n",
        "        self.env = env\n",
        "\n",
        "\n",
        "    def select_action(self, state, noise=0.1):\n",
        "\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "\n",
        "        # sample policy action\n",
        "        action = self.actor(state).cpu().data.numpy().flatten()\n",
        "        if noise != 0:\n",
        "            # add noise to action\n",
        "            action = (action + np.random.normal(0, noise, size=self.env.action_space.shape[0]))\n",
        "\n",
        "        # clip noise added action to lie within action space\n",
        "        return action.clip(self.env.action_space.low, self.env.action_space.high)\n",
        "\n",
        "\n",
        "    def train(self, replay_buffer, iterations, weight, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "\n",
        "        for it in range(iterations):\n",
        "\n",
        "            # retrieve sample from replay buffer\n",
        "            s, a, s_, r, d = replay_buffer.sample(batch_size)\n",
        "            state = torch.FloatTensor(s).to(device)\n",
        "            action = torch.FloatTensor(a).to(device)\n",
        "            next_state = torch.FloatTensor(s_).to(device)\n",
        "            done = torch.FloatTensor(1 - d).to(device)\n",
        "            reward = torch.FloatTensor(r).to(device)\n",
        "\n",
        "            # propose next action\n",
        "            noise = torch.FloatTensor(a).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # get target Q value\n",
        "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + (done * discount * target_Q).detach()\n",
        "\n",
        "            # regress Q-networks to target network\n",
        "            current_Q1, current_Q2 = self.critic(state, action)\n",
        "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            # train Forward looking network\n",
        "            pred_next_state = self.fork(state, action)\n",
        "            fork_loss = F.mse_loss(pred_next_state, next_state)\n",
        "\n",
        "            self.fork_optimizer.zero_grad()\n",
        "            fork_loss.backward()\n",
        "            self.fork_optimizer.step()\n",
        "\n",
        "            # update actor every other iteration\n",
        "            if it % policy_freq == 0:\n",
        "\n",
        "                # get base actor loss\n",
        "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "                # apply loss from future actions iff FORK is accurate enough\n",
        "                if fork_loss < 0.020:\n",
        "\n",
        "                  # apply weighted loss of action in s'\n",
        "                  pred_next_state = self.fork(state, action).clamp(-self.max_state,self.max_state)\n",
        "                  next_action = self.actor(pred_next_state.detach()) * self.max_action\n",
        "                  actor_loss -= weight * self.critic.Q1(pred_next_state, next_action).mean()\n",
        "\n",
        "                  # apply weighted loss of action in s''\n",
        "                  pred_next_state = self.fork(pred_next_state, next_action).clamp(-self.max_state,self.max_state)\n",
        "                  next_action = self.actor(pred_next_state.detach()) * self.max_action\n",
        "                  actor_loss -= 0.5 * weight * self.critic.Q1(pred_next_state, next_action).mean()\n",
        "\n",
        "                # train actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "\n",
        "                # update target networks\n",
        "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TsXOUNJ5AS61"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "env = gym.wrappers.record_video.RecordVideo(env, \"./video\", episode_trigger = lambda ep_id: ep_id%video_every == 0)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "max_state = float(env.observation_space.high[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC8Qx7GUBXJh",
        "outputId": "c243983f-8dd9-4a11-fe19-fbf022b56ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The environment has 24 observations and the agent can take 4 actions\n",
            "The device is: cuda\n",
            "It's recommended to train on the cpu for this\n"
          ]
        }
      ],
      "source": [
        "print('The environment has {} observations and the agent can take {} actions'.format(state_dim, act_dim))\n",
        "print('The device is: {}'.format(device))\n",
        "\n",
        "if device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9vsUdjvBZI9",
        "outputId": "4690b83c-917e-47eb-9be0-adc0a3f378b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "\n",
        "# initialise agent\n",
        "agent = Agent(state_dim, act_dim, max_action, max_state, env)\n",
        "done = False\n",
        "# initialise replay buffer\n",
        "replay_buffer = ReplayBuffer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ee20spf3BbH1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "39c45a23-a39f-4020-9aab-c911d7c26bbf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-5c6b003837e7>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0;31m# get results of action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m           \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m           \u001b[0mep_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/record_video.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mtruncateds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0minfos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         ) = step_api_compatibility(self.env.step(action), True, self.is_vector_env)\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mBeginContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         if (\n\u001b[1;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhull\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# logging variables\n",
        "ep_reward = 0\n",
        "reward_list = []\n",
        "plot_data = []\n",
        "log_f = open(\"agent-log.txt\",\"w+\")\n",
        "\n",
        "# max timestep per episode\n",
        "max_timesteps = 2000\n",
        "\n",
        "# number of timesteps in observation stage\n",
        "random_timesteps = 25000\n",
        "\n",
        "state = env.reset()\n",
        "\n",
        "i = 0\n",
        "episode = 0\n",
        "expcount = 0\n",
        "\n",
        "while episode < max_episodes:\n",
        "\n",
        "      episode += 1\n",
        "      temp_replay_buffer = []\n",
        "\n",
        "      for t in range(max_timesteps):\n",
        "\n",
        "          i += 1\n",
        "\n",
        "          if i < random_timesteps:\n",
        "            # sample random action in observation stage\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "          else:\n",
        "            # sample policy action outside observation stage\n",
        "            action = agent.select_action(state, 0.1)\n",
        "\n",
        "          # get results of action\n",
        "          next_state, reward, done, _ = env.step(action)\n",
        "          ep_reward += reward\n",
        "\n",
        "          # apply reward scaling\n",
        "          if reward == -100:\n",
        "                add_reward = -1\n",
        "                reward = -5\n",
        "                expcount += 1\n",
        "          else:\n",
        "                add_reward = 0\n",
        "                reward = 5 * reward\n",
        "\n",
        "          temp_replay_buffer.append((state, action, next_state, reward, done))\n",
        "\n",
        "          state = next_state\n",
        "\n",
        "          # at end of episode\n",
        "          if done or t==(max_timesteps-1):\n",
        "\n",
        "            # trains unsuccesful episodes at ratio 5:1\n",
        "            totrain = 0\n",
        "            if add_reward == -1 or ep_reward < 250:\n",
        "                totrain = 1\n",
        "                for temp in temp_replay_buffer:\n",
        "                    replay_buffer.add(temp)\n",
        "            elif expcount > 0 and np.random.rand() > 0.5:\n",
        "                totrain = 1\n",
        "                expcount -= 10\n",
        "                for temp in temp_replay_buffer:\n",
        "                    replay_buffer.add(temp)\n",
        "\n",
        "            reward_list.append(ep_reward)\n",
        "            avg_reward = np.mean(reward_list[-100:])\n",
        "\n",
        "            if i > random_timesteps:\n",
        "\n",
        "                # weight to be applied to actions on FORK predicted states\n",
        "                weight = 1 - np.clip(np.mean(avg_reward)/300, 0, 1)\n",
        "                if totrain == 1:\n",
        "                    agent.train(replay_buffer, t, weight)\n",
        "                else:\n",
        "                    agent.train(replay_buffer, 100, weight)\n",
        "                totrain = 0\n",
        "\n",
        "\n",
        "            done = False\n",
        "\n",
        "            state = env.reset()\n",
        "\n",
        "            break\n",
        "\n",
        "\n",
        "      log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
        "      log_f.flush()\n",
        "      ep_reward = 0\n",
        "\n",
        "      # print reward data every so often - add a graph like this in your report\n",
        "      if episode % plot_interval == 0:\n",
        "          plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "          reward_list = []\n",
        "          plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
        "          plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
        "          plt.xlabel('Episode number')\n",
        "          plt.ylabel('Episode reward')\n",
        "          plt.show()\n",
        "          disp.clear_output(wait=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0AYfMGr8-G7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}